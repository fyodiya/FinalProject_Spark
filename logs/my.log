22/08/26 12:39:02 ERROR netty.Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@26cb8616 rejected from java.util.concurrent.ThreadPoolExecutor@2b12d009[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
	at scala.collection.immutable.Vector.foreach(Vector.scala:1856)
	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:68)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
22/08/26 20:04:37 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/28 13:41:27 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/28 14:47:30 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/28 18:37:41 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/28 21:37:29 WARN util.Instrumentation: [b50a8c0d] regParam is zero, which might cause numerical instability and overfitting.
22/08/28 21:37:29 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/28 21:37:29 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/28 21:37:29 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/28 21:38:03 WARN util.Instrumentation: [791b15c7] regParam is zero, which might cause numerical instability and overfitting.
22/08/28 21:38:03 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/28 21:38:03 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/28 21:38:03 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/28 22:29:05 ERROR util.Instrumentation: java.lang.IllegalArgumentException: features does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:123)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:111)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchemaImpl(CrossValidator.scala:80)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:212)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$1(CrossValidator.scala:139)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:137)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:62)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:9)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:9)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 22:30:19 ERROR util.Instrumentation: java.lang.IllegalArgumentException: features does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:123)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:111)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchemaImpl(CrossValidator.scala:80)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:212)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$1(CrossValidator.scala:139)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:137)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:63)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:9)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:9)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 22:32:22 ERROR util.Instrumentation: java.lang.IllegalArgumentException: features does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:123)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:111)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchemaImpl(CrossValidator.scala:80)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:212)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$1(CrossValidator.scala:139)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:137)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:63)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:9)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:9)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 22:43:41 ERROR util.Instrumentation: java.lang.IllegalArgumentException: features does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:123)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:111)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchemaImpl(CrossValidator.scala:80)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:212)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$1(CrossValidator.scala:139)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:137)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:63)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:9)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:9)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 22:52:17 ERROR util.Instrumentation: java.lang.IllegalArgumentException: features does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:123)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:111)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchemaImpl(CrossValidator.scala:80)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:212)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$1(CrossValidator.scala:139)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:137)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 22:58:08 ERROR util.Instrumentation: java.lang.IllegalArgumentException: features does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:123)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:111)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchemaImpl(CrossValidator.scala:80)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:212)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$1(CrossValidator.scala:139)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:137)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 23:02:12 ERROR util.Instrumentation: java.lang.IllegalArgumentException: features does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:123)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:111)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 23:03:09 ERROR util.Instrumentation: java.lang.IllegalArgumentException: features does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:123)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:111)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 23:05:27 ERROR util.Instrumentation: java.lang.IllegalArgumentException: features does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:123)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:111)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 23:09:29 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Data type string of column date is not supported.
Data type string of column ticker is not supported.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:168)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 23:23:51 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Data type string of column date is not supported.
Data type string of column ticker is not supported.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:168)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 23:24:52 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Data type string of column date is not supported.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:168)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)

22/08/28 23:26:02 WARN util.Instrumentation: [5debe86c] regParam is zero, which might cause numerical instability and overfitting.
22/08/28 23:26:03 ERROR executor.Executor: Exception in task 0.0 in stage 5.0 (TID 4)
org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more
22/08/28 23:26:03 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 5.0 (TID 4) (Fyodiya executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more

22/08/28 23:26:03 ERROR scheduler.TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
22/08/28 23:26:03 ERROR util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 4) (Fyodiya executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)
	at org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1177)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)
	at org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)
	at org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:451)
	at org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:345)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:327)
	at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)
Caused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more

22/08/28 23:26:03 ERROR util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 4) (Fyodiya executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)
	at org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1177)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)
	at org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)
	at org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:451)
	at org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:345)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:327)
	at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)
Caused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more

22/08/28 23:30:02 WARN util.Instrumentation: [bb03ef70] regParam is zero, which might cause numerical instability and overfitting.
22/08/28 23:30:02 ERROR executor.Executor: Exception in task 0.0 in stage 5.0 (TID 4)
org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more
22/08/28 23:30:02 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 5.0 (TID 4) (Fyodiya executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more

22/08/28 23:30:02 ERROR scheduler.TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
22/08/28 23:30:02 ERROR util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 4) (Fyodiya executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)
	at org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1177)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)
	at org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)
	at org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:451)
	at org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:345)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:327)
	at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)
Caused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more

22/08/28 23:30:02 ERROR util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 4) (Fyodiya executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.immutable.List.foreach(List.scala:333)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)
	at org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1177)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)
	at org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)
	at org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:451)
	at org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:345)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:327)
	at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:184)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:68)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:11)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:11)
	at PredictionModel.main(PredictionModel.scala)
Caused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$2596/1775829753: (struct<date:double,open:double,close:double,dailyReturn:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:252)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)
	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:645)
	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:642)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1293)
	at scala.collection.IterableOnceOps.aggregate(IterableOnce.scala:1086)
	at scala.collection.IterableOnceOps.aggregate$(IterableOnce.scala:1086)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1293)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)
	at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:250)
	... 26 more

22/08/29 00:38:09 WARN feature.StringIndexerModel: Input column date does not exist during transformation. Skip StringIndexerModel for this column.
22/08/29 00:39:34 ERROR util.Instrumentation: java.lang.IllegalArgumentException: dateInd does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transformSchema$1(VectorAssembler.scala:161)
	at scala.collection.ArrayOps$.flatMap$extension(ArrayOps.scala:964)
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:160)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:84)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 00:44:03 ERROR util.Instrumentation: java.lang.IllegalArgumentException: dateEnc does not exist. Available: date, dateInd
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.Map$Map2.getOrElse(Map.scala:307)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transformSchema$1(VectorAssembler.scala:161)
	at scala.collection.ArrayOps$.flatMap$extension(ArrayOps.scala:964)
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:160)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:84)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 00:44:35 ERROR util.Instrumentation: java.lang.IllegalArgumentException: open does not exist. Available: date, dateInd
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.Map$Map2.getOrElse(Map.scala:307)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transformSchema$1(VectorAssembler.scala:161)
	at scala.collection.ArrayOps$.flatMap$extension(ArrayOps.scala:964)
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:160)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:84)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 00:56:48 ERROR util.Instrumentation: java.lang.IllegalArgumentException: tickerInd does not exist. Available: date, open, high, low, close, volume, ticker, dailyReturn, dateInd
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:683)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transformSchema$1(VectorAssembler.scala:161)
	at scala.collection.ArrayOps$.flatMap$extension(ArrayOps.scala:964)
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:160)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:95)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 00:57:31 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Data type string of column ticker is not supported.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:168)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:134)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:87)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 00:57:56 WARN util.Instrumentation: [6adbb30b] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 00:57:56 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 00:57:56 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 00:57:56 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 00:57:56 WARN util.Instrumentation: [6adbb30b] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 11:11:49 WARN util.Instrumentation: [fde025b7] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 11:11:49 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 11:11:49 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 11:11:49 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 11:11:49 WARN util.Instrumentation: [fde025b7] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 11:29:51 WARN util.Instrumentation: [4f255960] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 11:29:52 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 11:29:52 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 11:29:52 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 11:29:52 WARN util.Instrumentation: [4f255960] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 11:29:54 WARN util.Instrumentation: [bb140cd2] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 11:29:54 WARN util.Instrumentation: [bb140cd2] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 11:29:55 WARN util.Instrumentation: [59707c92] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 11:29:55 WARN util.Instrumentation: [59707c92] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 11:32:36 ERROR util.Instrumentation: java.lang.IllegalArgumentException: requirement failed: Output column dateInd already exists.
	at scala.Predef$.require(Predef.scala:337)
	at org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformField(StringIndexer.scala:110)
	at org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:126)
	at scala.collection.ArrayOps$.flatMap$extension(ArrayOps.scala:964)
	at org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)
	at org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)
	at org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)
	at org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchemaImpl(CrossValidator.scala:80)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:212)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$1(CrossValidator.scala:139)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:137)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:72)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 11:41:32 WARN util.Instrumentation: [6a4194fd] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 11:41:33 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 11:41:33 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 11:41:33 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 11:41:33 WARN util.Instrumentation: [6a4194fd] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 11:41:34 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/29 11:41:35 WARN util.Instrumentation: [f613a634] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 11:41:35 WARN util.Instrumentation: [f613a634] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 11:41:35 WARN util.Instrumentation: [ded349a2] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 11:41:35 WARN util.Instrumentation: [ded349a2] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 11:55:22 WARN util.Instrumentation: [71cb5ccd] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 11:55:22 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 11:55:22 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 11:55:23 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 11:55:23 WARN util.Instrumentation: [71cb5ccd] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 11:55:23 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
22/08/29 11:55:23 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
22/08/29 11:55:24 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Output column features already exists.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:171)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchemaImpl(TrainValidationSplit.scala:68)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchema(TrainValidationSplit.scala:182)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$1(TrainValidationSplit.scala:123)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:121)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:92)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 11:57:56 WARN util.Instrumentation: [d9489cdf] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 11:57:57 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 11:57:57 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 11:57:57 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 11:57:57 WARN util.Instrumentation: [d9489cdf] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 11:57:57 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
22/08/29 11:57:57 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
22/08/29 11:57:58 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Output column features already exists.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:171)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchemaImpl(TrainValidationSplit.scala:68)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchema(TrainValidationSplit.scala:182)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$1(TrainValidationSplit.scala:123)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:121)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:92)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 12:04:17 WARN util.Instrumentation: [fe29cb51] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 12:04:18 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 12:04:18 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 12:04:18 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 12:04:18 WARN util.Instrumentation: [fe29cb51] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 12:04:19 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
22/08/29 12:04:19 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
22/08/29 12:04:19 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Output column features already exists.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:171)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchemaImpl(TrainValidationSplit.scala:68)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchema(TrainValidationSplit.scala:182)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$1(TrainValidationSplit.scala:123)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:121)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:90)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 12:08:48 WARN util.Instrumentation: [76f00f95] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 12:08:48 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 12:08:48 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 12:08:48 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 12:08:48 WARN util.Instrumentation: [76f00f95] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 12:08:49 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Output column features already exists.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:171)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchemaImpl(TrainValidationSplit.scala:68)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchema(TrainValidationSplit.scala:182)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$1(TrainValidationSplit.scala:123)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:121)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:88)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 12:22:31 WARN util.Instrumentation: [c1a8c4db] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 12:22:32 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 12:22:32 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 12:22:32 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 12:22:32 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Output column features already exists.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:171)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchemaImpl(TrainValidationSplit.scala:68)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchema(TrainValidationSplit.scala:182)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$1(TrainValidationSplit.scala:123)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:121)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:89)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 12:26:38 WARN util.Instrumentation: [b7bae696] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 12:26:38 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 12:26:38 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 12:26:38 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 12:26:38 WARN util.Instrumentation: [b7bae696] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 12:26:39 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Output column features already exists.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:171)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchemaImpl(TrainValidationSplit.scala:68)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchema(TrainValidationSplit.scala:182)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$1(TrainValidationSplit.scala:123)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:121)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:88)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 12:27:47 WARN util.Instrumentation: [defaf08d] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 12:27:48 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 12:27:48 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 12:27:48 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 12:27:48 WARN util.Instrumentation: [defaf08d] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 12:27:48 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Output column features already exists.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:171)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchemaImpl(TrainValidationSplit.scala:68)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchema(TrainValidationSplit.scala:182)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$1(TrainValidationSplit.scala:123)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:121)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:85)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 12:28:57 WARN util.Instrumentation: [591f3494] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 12:28:57 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 12:28:57 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 12:28:57 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 12:28:57 WARN util.Instrumentation: [591f3494] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 12:28:58 ERROR util.Instrumentation: java.lang.IllegalArgumentException: Output column features already exists.
	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:171)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchemaImpl(TrainValidationSplit.scala:68)
	at org.apache.spark.ml.tuning.TrainValidationSplit.transformSchema(TrainValidationSplit.scala:182)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.TrainValidationSplit.$anonfun$fit$1(TrainValidationSplit.scala:123)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:121)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:85)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 12:30:47 WARN util.Instrumentation: [ccd64ce0] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 12:30:47 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 12:30:47 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 12:30:47 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 12:30:47 WARN util.Instrumentation: [ccd64ce0] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 12:30:51 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/29 12:38:23 WARN util.Instrumentation: [5d5ff0ed] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 12:38:23 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 12:38:23 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 12:38:23 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 12:38:23 WARN util.Instrumentation: [5d5ff0ed] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 12:38:28 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/29 12:38:45 WARN util.Instrumentation: [c2513201] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 12:38:45 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 12:38:45 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 12:38:45 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 12:38:45 WARN util.Instrumentation: [c2513201] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 12:38:46 ERROR util.Instrumentation: java.lang.IllegalArgumentException: requirement failed: Output column dateInd already exists.
	at scala.Predef$.require(Predef.scala:337)
	at org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformField(StringIndexer.scala:110)
	at org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:126)
	at scala.collection.ArrayOps$.flatMap$extension(ArrayOps.scala:964)
	at org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)
	at org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)
	at org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)
	at org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)
	at org.apache.spark.ml.Pipeline.$anonfun$transformSchema$4(Pipeline.scala:183)
	at scala.collection.ArrayOps$.foldLeft$extension(ArrayOps.scala:779)
	at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:183)
	at org.apache.spark.ml.tuning.ValidatorParams.$anonfun$transformSchemaImpl$2(ValidatorParams.scala:75)
	at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl(ValidatorParams.scala:74)
	at org.apache.spark.ml.tuning.ValidatorParams.transformSchemaImpl$(ValidatorParams.scala:70)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchemaImpl(CrossValidator.scala:80)
	at org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:212)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$1(CrossValidator.scala:139)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:210)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:137)
	at PredictionModel$.delayedEndpoint$PredictionModel$1(PredictionModel.scala:85)
	at PredictionModel$delayedInit$body.apply(PredictionModel.scala:10)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1(App.scala:76)
	at scala.App.$anonfun$main$1$adapted(App.scala:76)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
	at scala.App.main(App.scala:76)
	at scala.App.main$(App.scala:74)
	at PredictionModel$.main(PredictionModel.scala:10)
	at PredictionModel.main(PredictionModel.scala)

22/08/29 12:39:55 WARN util.Instrumentation: [a71e2b4d] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 12:39:55 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 12:39:55 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 12:39:55 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 18:30:52 WARN util.Instrumentation: [5802aae4] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 18:30:53 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 18:30:53 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 18:30:53 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 18:30:53 WARN util.Instrumentation: [5802aae4] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 18:30:55 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/29 18:31:35 WARN util.Instrumentation: [28b666b4] regParam is zero, which might cause numerical instability and overfitting.
22/08/29 18:31:35 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/29 18:31:35 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/29 18:31:35 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/29 18:31:35 WARN util.Instrumentation: [28b666b4] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/29 18:31:40 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/30 13:20:09 WARN util.Instrumentation: [e5a2d5d9] regParam is zero, which might cause numerical instability and overfitting.
22/08/30 13:20:09 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/30 13:20:09 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/30 13:20:09 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/30 13:20:09 WARN util.Instrumentation: [e5a2d5d9] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/30 13:20:15 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/30 13:22:21 WARN util.Instrumentation: [7c8b8262] regParam is zero, which might cause numerical instability and overfitting.
22/08/30 13:22:22 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/30 13:22:22 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/30 13:22:22 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/30 13:22:24 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/30 13:23:10 WARN util.Instrumentation: [7960b10a] regParam is zero, which might cause numerical instability and overfitting.
22/08/30 13:23:11 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/30 13:23:11 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/30 13:23:11 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/30 13:23:11 WARN util.Instrumentation: [7960b10a] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/30 13:23:11 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
22/08/30 13:24:12 WARN util.Instrumentation: [c8953bfe] regParam is zero, which might cause numerical instability and overfitting.
22/08/30 13:24:13 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
22/08/30 13:24:13 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS
22/08/30 13:24:13 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK
22/08/30 13:24:13 WARN util.Instrumentation: [c8953bfe] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.
22/08/30 13:24:15 WARN executor.ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
